---
title: "Foundations of Spectral Clustering"
author: "Dong Li"
date: "28 October 2016"
#date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation
Recently we read a [Science paper](http://science.sciencemag.org/content/353/6295/163) which adopts spectral clustering as the main method to find higher-order connectivity patterns. The efficiency and scalability of their algorithm are impressive. And We also found that the best player in this year's [Disease Module Identification DREAM Challenge](https://www.synapse.org/#!Synapse:syn6156761/wiki/400645) uses spectral clustering as one of their main techniques to identify modules, which achieved the best overall performance at all FDR levels. Check their write-up: [A Double Spectral Approach to DREAM 11 Subchallenge 3](https://www.synapse.org/#!Synapse:syn7349492/wiki/407359).

As a classical clustering method, spectral techniques are still promising. For many people including me, the basic questions remain as: what is spectral clustering and why it works?

## A starter dish about clustering

Before we step into spectral clustering, let's look at a simple example about clustering. Image we have a collection of two-dimensional data points looks like,
```{r, echo=T}
library(mlbench)
n <- 100
k <- 2
dat1 <- mlbench.2dnormals(n, cl=k, r=sqrt(k), sd=0.7)
plot(dat1$x,col=rep(1,n))
```

How many clusters are there? It is easy for human to say there are roughly two clusters:

```{r, echo=T}
plot(dat1$x,col=dat1$classes)
```

But how can we make the program know? We may need classical [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering). The idea of k-means is quite straightforward:

- 1) randomly picking $k$ nodes as centroids, 
- 2) assign each data points to the centroids which are the most "close", 
- 3) calculated the new centroids based on previous assignment and 
- 4) repeat 2-3 until the centroids keep unchanged. 

Code for 2) Assignment and 3) Update centroids:

```{r, echo=TRUE}
# Assign cluster labels given centroidids
Assigment <- function(dat,centroidids){
    n <- dim(dat)[1]
    k <- dim(centroidids)[1]
    classes <- rep(1,n)

    for(i in 1:n){
        #mindist <- dist(rbind(dat[i,],centroidids[1]))
        mindist <- norm(as.matrix(dat[i,]-centroidids[1]), type="F")
        for(j in 2:k){
            currentdis <- norm(as.matrix(dat[i,]-centroidids[j]), type="F")
            if( currentdis < mindist){
                mindist <- currentdis
                classes[i] <- j
            }
        }
    }
    classes
}

# Update centroidids given cluster labels
Update <- function(dat,classes,k){
    l <- dim(dat)[2]
    centroidids <- matrix(0, nrow=k,ncol=l)
    for(i in 1:k){
        centroidids[i,] <- colMeans(dat[which(classes==i),])
    }
    centroidids
}
```

The basic k-means seems to work well in previous example. We can see the procedure of how centroids move step by step:

```{r, echo=FALSE}
#kmeansStepbyStep
initids <- sample(n)[1:k]
par(mfrow=c(2,2))
cols <- rep(1,n)
plot(dat1$x, col=cols)
title('Initial status')
centroidids <- dat1$x[initids,]
classes <- Assigment(dat1$x, centroidids)
plot(dat1$x, col=classes)
points(centroidids, y = NULL, pch=19, cex=2, col=1:k)
title('The first assignment')
for(i in 3:4){
    centroidids <- Update(dat1$x, classes, k)
    classes <- Assigment(dat1$x, centroidids)
    plot(dat1$x, col=classes)
    points(centroidids, y = NULL, pch=19, cex=2, col=1:k)
    title(paste('The assignment procedure: ',(i-1)))
}
```

Note that the above implementation of Kmeans is merely for illustration. A good implementation should consider the performance. One of the main methods to improve the performance of Kmeans is using vectorization instead of for-loops everywhere. See another [post](HPCP/hpc.html) for details about how to vectorize the distance calculation in `Assigment`. Kmeans is also a special form of more general model [expectationâ€“maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), where the E-step is the `Assigment` procedure and M-step is `Update` procedure. 

But when the data points are not so straightforward (linearly separable), such as spirals, where k-means may fail:

```{r, echo=TRUE}
dat2 <- mlbench.spirals(100,1,0.025)
par(mfrow=c(1,2))
plot(dat2$x)
title('data points')

#we call the system function kmeans this time
km <- kmeans(dat2$x, centers=k)
plot(dat2$x, col=km$cluster)
title('Kmeans result')
```

We will see spectral clustering can work well here.

## Spectral clustering algorithms
[Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering) makes use of spectrum(eigenvalues) of the similarity matrix to perform data analysis. One simple form of the algorithm goes like

- Make the affinity matrix $A$ based on the similarity matrix $W$.
- Make the diagonal matrix $D$ where $D_{ii}=\sum_jA_{ij}$, and the unnormalized Laplacian matrix $L=D-A$ or normalized Laplacian matrix $L_{sym}=D^{-1/2}LD^{-1/2}$.
- Find the first k eigenvectors $x_1,x_2,...x_k$ of $L$ and form the new matrix $X=[x_1x_2...x_k]\in \mathbb{R}^{n\times k}$.
- Conduct K-means on X.

Note that the affinity matrix is not always the similarity matrix $W$. For some problems, we need to consider the local neighborhood relationships. k-nearest neighbour filter is a common choice:

```{r, echo=TRUE}
#Similarity matrix is defined by Gaussian Kernel distance
S <- matrix(rep(NA,n^2), ncol=n)
  for(i in 1:n) {
    for(j in 1:n) {
      S[i,j] <- exp(- 1 * norm(as.matrix(dat2$x[i,]-dat2$x[j,]), type="F"))
    }
}

#k-nearest neighorhood to filter affinity
make.affinity <- function(S, n.neighboors=3) {
  N <- length(S[,1])
  if (n.neighboors >= N) {  # fully connected
    A <- S
  } else {
    A <- matrix(rep(0,N^2), ncol=N)
    for(i in 1:N) { # for each line
      # only connect to those points with larger similarity 
      best.similarities <- sort(S[i,], decreasing=TRUE,index.return = TRUE)
      for (j in best.similarities$ix[1:n.neighboors]) {
        A[i,j] <- S[i,j]
        A[j,i] <- S[i,j] # to make an undirected graph, ie, the matrix becomes symmetric
      }
    }
  }
  A  
}

##The real code for spectral clustering is quite simple:
A <- make.affinity(S)
d <- apply(A, 1, sum)
L <- diag(d)-A                       # unnormalized version
#L <- diag(d^-0.5)%*%L%*% diag(d^-0.5) # normalized version
evL <- eigen(L,symmetric=TRUE)	# evL$values is decreasing sorted when symmetric=TRUE
# pick the first k first k eigenvectors (corresponding k smallest) as data points in spectral space
Z <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]
spc <- kmeans(Z, centers=k)
plot(dat2$x, col=spc$cluster)
title('Spectral clustering result')
```

And all these can be done in one line with package [kernlab](https://cran.r-project.org/web/packages/kernlab).

```{r}
library(kernlab)
skm=specc(dat2$x,centers=2)
plot(dat2$x, col=skm)
title('Spectral clustering result by kernlab')
```

Probably you have noticed that the matrix $Z\in \mathbb{R}^{n\times k}$ we really perform Kmeans clustering on can be viewed as another useful representation of original dataset $X\in \mathbb{R}^{n\times d}$. For some applications such as document classification/clustering $d>>k$, thus spectral help to do dimensionality reduction. What is more, data points become linearly separable in this so called spectral space.

```{r}
library(kernlab)
plot(Z)
title('Data points in spectral space')
```

## Why spectral clustering works?

At the first sight, we may feel amazing. Why it works? Why we pick exactly $k$ eigenvectors and why they are separable under this representation? Following the logic of [Luxburg's Tutorial](http://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf), the principle of explaining why Eigendecomposition can be used to do clustering is to **establish an equivalence between Eigendecomposition based procedure and clustering data points**. We only explore it from the Graph cut perspective based on normalized Laplacian.

The essence of clustering is to separate data points based on their similarities. Given the similarity graph (where nodes are data points and edges are similarities) of these points, it aims to find a partition that edges within the same cluster are dense and between different clusters are sparse. This problem has been intensively studied in graph theory.

Check [Cut](https://en.wikipedia.org/wiki/Cut_%28graph_theory%29) to distinguish confusing concepts of cut, cut-set and Minimum cut. Given the adjacency matrix $W$ of a graph $G=(V,E)$, we define the following metric for two disjoint subsets $A,B\in V$:
$$cut(A,B)=\sum_{i\in A,j\in B}w_{ij}$$
and $\bar{A}$ as the [complement](https://en.wikipedia.org/wiki/Complement_%28set_theory%29) of $A$. For given $k$ clusters, we want to find a partition $A_1,A_2,...A_k$ that minimizes
$$Cut(A_1,A_2,...A_k)=\frac{1}{2}\sum_{i=1}^kcut(A_i,\bar{A_i})$$
But simply minimizing this metric would lead to large clusters. Several improvements were proposed and we mainly discuss normalized cut [Ncut](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf):
$$Ncut(A_1,A_2,...A_k)=\sum_{i=1}^k\frac{cut(A_i,\bar{A_i})}{vol(A_i)}$$
where $vol(A)=\sum_{i\in A,j}w_{i,j}=\sum_{i\in A}d_i$ and $d_i$ is the connectivity (degree) of $i$.

Next, we want to derive normalized spectral clustering as relaxation of minimizing Ncut. Considering the simplest case When $k=2$, there are only $A$ and $\bar{A}$, we define the cluster indicator $f$ (this is the key idea) as
$$f_i=\left\{
                \begin{array}{ll}
                  \sqrt{\frac{vol(\bar{A})}{vol(A)}}&if\quad v_i\in A\\
                  -\sqrt{\frac{vol(A)}{vol(\bar{A})}}&if\quad v_i\in \bar{A}
                \end{array}
              \right.$$

Note that $f^TD{\bf 1}=0$. As we know $D_{ii}=d_i$, thus 
$$\begin{array}{ll}
\sum_if_id_i=\sqrt{vol(\bar{A})/vol(A)}\sum_{i\in A}d_i-\sqrt{vol(A)/vol(\bar{A})}\sum_{j\in \bar{A}}d_j\\
=\sqrt{vol(\bar{A})/vol(A)}vol(A)-\sqrt{vol(A)/vol(\bar{A})}vol(\bar{A})=0
\end{array}$$.
And we also have $f'Df=vol(V)$ since
$$\begin{array}{ll}
f'Df=\sum_if_i^2d_i=\sum_{i\in A}d_i\frac{vol(\bar{A})}{vol(A)}+\sum_{i\in \bar{A}}d_i\frac{vol(A)}{vol(\bar{A})}\\
=vol(\bar{A})+vol(A)=vol(V)
\end{array}$$

For every $f$ we have $f'Lf=1/2\sum_{ij}(f_i-f_j)^2$, and for the specific $f$ defined above, we have 
$$\begin{array}{ll}
f'Lf=\frac{1}{2}\sum_{i\in A,j\in \bar{A}}w_{ij}\Big(\sqrt{\frac{vol(\bar{A})}{vol(A)}}+\sqrt{\frac{vol(A)}{vol(\bar{A})}}\Big)^2+
\frac{1}{2}\sum_{i\in \bar{A},j\in A}w_{ij}\Big(-\sqrt{\frac{vol(A)}{vol(\bar{A})}}-\sqrt{\frac{vol(\bar{A})}{vol(A)}}\Big)^2\\
=cut(A,\bar{A})\Big(\frac{vol(A)}{vol(\bar{A})}+\frac{vol(\bar{A})}{vol(A)}+2\Big)\\
=cut(A,\bar{A})\Big(\frac{vol(A)+vol(\bar{A})}{vol(\bar{A})}+\frac{vol(\bar{A})+vol(A)}{vol(A)}\Big)\\
=vol(V)Ncut(A,\bar{A})
\end{array}$$

Because $vol(V)$ is constant for given graph, the equivalence problem of minimize Ncut is to minimize $f'Lf$ w.r.t $f$. Unfortunately it is a discrete optimization problem.

If we relax the vector $f$ to real number space we have the following optimization problem
$$\min_{f\in \mathbb{R}^n} f'Lf\quad s.t.\quad f^TD\perp\mathbb{1},f'Df=vol(V)$$

Furthermore we make $g=D^{1/2}f$, the problem becomes
$$\min_{g\in \mathbb{R}^n} g'L_{sym}g\quad s.t.\quad g\perp D^{1/2}\mathbb{1},\|g\|^2=vol(V)$$
where $L_{sym}=D^{-1/2}LD^{-1/2}$. According to [Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient), the solution is given by the second eigenvector of $L_{sym}$. That means as long as we get $f$ by eigenvector of $L_{sym}$, we get a partition which make minimize Ncut thus get the clusters. Refer to [Luxburg's Tutorial](http://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf) when $k>2$.

## References
This document is written in R Markdown. Want to write one like this? See [Writing reproducible reports in R ](https://nicercode.github.io/guides/reports/). 

A close topic was given by JoÃ£o Neto at [Spectral Clustering](http://www.di.fc.ul.pt/~jpn/r/spectralclustering/spectralclustering.html). 

A nice tutorial of spectral clustering was given by Luxburg at [A Tutorial on Spectral Clustering](http://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf).

A popular Chinese machine learning blog by pluskid also contains [spectral clustering](http://blog.pluskid.org/?p=287).

--
Last update: `r Sys.setlocale("LC_TIME", "English"); format(Sys.Date(), "%B %d, %Y")`
