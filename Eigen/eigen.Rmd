---
title: "Spectral Methods For Community Detection"
subtitle: "Graph partitioning, modularity maximization and eigenvectors"
author: "Dong Li"
date: "June 26, 2017"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation
[Mark Newman](http://www-personal.umich.edu/~mejn/) is a leading research in the field of complex networks. Because of his fundamental contribution, networks has been a hot topic for years. Reading Mark Newman's papers is always enjoyable since they are easy to understand, but filled with insights. The fluent style and clear equations sometimes make me wrongly believe anyone could do that kind of research...Anyway, there are three papers (all sole author) talking about the same thing: the spectral algorithm for community detection.

 - [[PNAS06](https://arxiv.org/pdf/physics/0602124.pdf)] Modularity and community structure in networks. PNAS, 2006.
 - [[PRE06](https://arxiv.org/pdf/physics/0605087.pdf)] Finding community structure in networks using the eigenvectors of matrices. Physical review E, 2006.
 - [[PRE13](https://arxiv.org/pdf/1307.7729.pdf)] Spectral methods for community detection and graph partitioning. Physical Review E, 2013.

Here I just follow these papers and explore the relationships between graph partitioning, modularity maximization, and eigenvectors.

# Introduction

Discovering groups in networks is of importance in general. As the network science emerges, finding community structure becomes a hot topic. But (theoretical) computer science people had been doing a similar thing before that: graph partitioning. At the front part of PNAS06, Newman highlighted the difference by the following aspects.   

|     | Graph partitioning  | Community detection |
|-----+--------------------+------------------------|
| Name | Graph   | Network      |
| Type | Abstract | Concrete |
| Examples | Random graph   | Social or biological networks |
| Objective | minimum cut   | maximum modularity |
| Researchers | Computer scientists, mathmaticians| Physicists, biologists, applied mathematicians |
| Differences | Known number and size of groups| Unknown number and size of groups |

Interestingly, in PRE13, Newman established equivalence between them. Specifically, he concluded that the spectral algorithms for 1) community detection by modularity maximization, 2) community detection by statistical inference and 3) normalized-cut graph partitioning are identical. I will not involve 2) due to my limited understanding. Here we have a look at how 1) and 3) are identical.

# Spectral algorithm

Modularity was proposed by Newman as criteria of grouping, defined as the fraction of edges within groups minus the expected fraction of such edges in a randomized null model of the network. Given the adjacency matrix $A$, $k_i$ is the degree of vertex $i$, $m$ is the total number of edges, modularity $Q$ is expressed as:
$$Q = {1\over2m}
    \sum_{ij} \biggl[ A_{ij} - {k_ik_j\over2m} \biggr] \delta_{g_ig_j},$$
where the Kronecker delta $\delta_{g_ig_j}=1$ when $i$ and $j$ are in the same group.

Consider there are only two groups. We define variables $s_i$ for each vertex and make $s_i=1$ if $i$ belongs to group 1 and $s_i=-1$ for group 2. Then $\delta_{g_ig_j} = 1/2(s_i s_j + 1)$. The modularity can be rewritten as:
$$Q = {1\over4m} \sum_{ij} \biggl[ A_{ij} - {k_ik_j\over2m} \biggr] (s_is_j+1).$$
The middle part $B_{ij}= A_{ij} - {k_ik_j\over2m} $ is the element from a so-called **modularity matrix**. Then the modularity is further rewritten as:
$$Q = {1\over4m} \sum_{ij} B_{ij} (s_is_j+1)
  = {1\over4m} \sum_{ij} B_{ij} s_i s_j,$$

as $\sum_j B_{ij} = \sum_j A_{ij} - {k_i\over2m} \sum_j k_j= k_i - {k_i\over2m} 2m = 0$.

Since $s_i$ are discrete values, maximizing Q is essentially a combinatorial problem. It is common to relax $\bf s$ to the real values. We are now solving an approximation which gives good results in practice. Also, note imposing constraints on $s_i$ is necessary otherwise we get an unbounded problem. The most commonly used L2-norm constraint $\sum_is_i=1$ actually limits any $s_i$ in the range $-\sqrt{n}\leq s_i \leq \sqrt{n}$. The problem is formally written as
$$\textrm{maximize}_{\bf s}\ Q={\bf s}^\top B {\bf s}\quad s.t.\quad \|{\bf s}\|_2=1$$
This problem is known as **spectral matching**, or **Rayleigh quotient** or even **PCA**, and the global optimum is given by the leading eigenvector of $B$. A simple [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier) would give the solution. Newman used another logic to conclude the leading eigenvector of $B$ in PNAS06, which partly explain why eigenvector maximizes it. After getting the leading eigenvector $\bf u$, we divide the vertices into two groups according to the signs of elements in $\bf u$. i.e., $i$ belongs to group 1 if $u_i>0$ if and group 2 for $u_i<0$.
  

# The equivalence
### Modularity maximization
In PRE13 Newman introduced a hyperellipsoid form $\sum_i a_i s_i^2 = \sum_i a_i$ for any set of nonnegative constants $a_i$. The above constraint should at least allow $s_i=\pm1$ in the original ``unrelaxed'' problem. When we set $a_i=k_i$, we have the constraint:
$$\sum_i k_i s_i^2 = 2m$$
The maximum of $Q$ is given by the derivative of a Lagrangian function:
$${\partial\over\partial s_i} \biggl[ \sum_{ij} B_{ij} s_i s_j
  - \lambda \sum_i k_i s_i^2 \biggr] = 0.$$
which can be simplified as
$$\sum_j B_{ij} s_j = \lambda k_i s_i,$$

If we replace the original form of $B_{ij}$ in the above equation, we get
$$\sum_j A_{ij} s_j = k_i \biggl(\lambda s_i + {1\over2m}\sum_j k_j s_j\biggr),$$
or in matrix notation as
$${\bf A}{\bf s} = {\bf D} \biggl(\lambda {\bf s} + {{\bf k}^\top{\bf s}\over2m}
                 {\bf 1} \biggr).$$
Note ${\bf A 1}={\bf D 1}={\bf k}$ and ${\bf k}^\top{\bf 1}=2m$, multiply above equation by ${\bf 1}^\top$ to get $\lambda{\bf k}^\top{\bf s}=0$.  Since we assuming there exists a nontrivial eigenvalue $\lambda >0$, ${\bf k}^\top{\bf s}=0$. Combining these we have
$${\bf A}{\bf s} = \lambda {\bf D} {\bf s}.
$$
This is also called generalized eigenvector equation. We can see that  that the uniform vector ${\bf 1}$ is an eigenvector, in other words all values in $\bf s$ are postive in this case. But it fails to satisigy ${\bf k}^\top{\bf s}=0$. Alternatively we choose the eigenvector corresponding to the second most positive eigenvalue.

We define a rescaled vector ${\bf u} = {\bf D}^{1/2}{\bf s}$, i.e., ${\bf s}={\bf D}^{-1/2}{\bf u}$. Then 
$$\bigl( {\bf D}^{-1/2}{\bf A}{\bf D}^{-1/2} \bigr) {\bf u} = \lambda{\bf u}.$$
We almost get there. The matrix $${\bf L} = {\bf D}^{-1/2}{\bf A}{\bf D}^{-1/2}$$ looks so familiar...yes, the normalized Laplacian. [This post](http://www.cs.bham.ac.uk/~dxl466/st/spectr.html) may helps a bit about the understanding on normalized-cut graph partitioning.

### Graph partitioning
Let us now recall how traditional graph partitioning does. It is required the cut size $R$-the number of edges running between parts-is minimized. In practise we define a normalized cut $Ncut$: $$Ncut=\frac{R}{\kappa_1\kappa_2}$$ 
where $\kappa_1=\sum_{i\in 1,j}w_{i,j}=\sum_{i\in 1}k_i$ is the sum of degrees of vertices in group 1, also called the volume in somewhere else.

We still use the indicating variables $s_i$ for each vertex, but slightly different to make $s_i=\sqrt{\kappa_2/\kappa_1}$ if $i$ belongs to group 1 and $s_i=-\sqrt{\kappa_2/\kappa_1}$ for group 2. Note that
$${\bf k}^\top{\bf s} = \sum_i k_i s_i = \sqrt{\kappa_2\over\kappa_1} \sum_{i\in1} k_i- \sqrt{\kappa_1\over\kappa_2} \sum_{i\in2} k_i= \sqrt{\kappa_2\kappa_1} - \sqrt{\kappa_1\kappa_2} = 0,$$
and
$${\bf s}^\top{\bf D}{\bf s} = \sum_i k_i s_i^2= {\kappa_2\over\kappa_1} \sum_{i\in1} k_i + {\kappa_1\over\kappa_2} \sum_{i\in2} k_i = \kappa_2 + \kappa_1 = 2m.$$
It is a trick to construct what we need like this:
$$s_i + \sqrt{\kappa_1\over\kappa_2}
  = {2m\over\sqrt{\kappa_1\kappa_2}}\,\delta_{g_i,1},$$
Similarly,
$$
s_i - \sqrt{\kappa_2\over\kappa_1}
  = - {2m\over\sqrt{\kappa_1\kappa_2}}\,\delta_{g_i,2} .
$$
Using these results we have
$$\sum_{ij} A_{ij} \biggl( s_i + \sqrt{\kappa_1\over\kappa_2} \biggr)\biggl( s_j - \sqrt{\kappa_2\over\kappa_1} \biggr)= -{(2m)^2\over\kappa_1\kappa_2} \sum_{ij} A_{ij}\,\delta_{g_i,1}\delta_{g_j,2}= -{(2m)^2\over\kappa_1\kappa_2} R,$$

Writing the left side into matrix form:
$$
\biggl( {\bf s} + \sqrt{\kappa_1\over\kappa_2}{\bf 1} \biggr)^\top {\bf A}
\biggl( {\bf s} - \sqrt{\kappa_2\over\kappa_1}{\bf 1} \biggr)
  = {\bf s}^\top{\bf A}{\bf s} - 2m,
$$
since ${\bf k} = {\bf A}{\bf 1}$,${\bf 1}^\top{\bf A 1} = 2m$ and ${\bf k^\top s}=0$.

Combing these we have the following matrix form:
$${R\over\kappa_1\kappa_2} = {2m - {\bf s}^\top{\bf A s}\over(2m)^2}.$$
Minimizing $Ncut$ is equivalent to maximizing ${\bf s}^\top{\bf A s}$. Like before, we relax $s_i$ to real values subject to constraints from previous derivation: ${\bf k^\top s}=0$ and ${\bf s}^\top{\bf D}{\bf s}=2m$. The problem can be solved by introducing Lagrange multipliers $\lambda,\mu$ for the two constraints and differentiating, which gives
$$
{\bf A s} = \lambda{\bf D s} + \mu{\bf k}.
$$
Multiplying on the left by ${\bf 1}^\top$ and making use of ${\bf 1}^\top{\bf A}
= {\bf 1}^\top{\bf D} ={\bf k}^\top$ gives
$$
{\bf k}^\top{\bf s} = \lambda {\bf k}^\top{\bf s} + 2m\mu,
$$
which implies that $\mu=0$ because  ${\bf k^\top s}=0$. Thus we finally get
$$
{\bf A s} = \lambda{\bf D s},
$$
which is exactly the same as in previous section *Modularity maximization*...Bingo!

# Implementation

[igraph](http://igraph.org/redirect.html) package implemented most community detection algorithms. It uses C as core language, R and Python interface languages, which considers both the efficiency and convenience. It even contains some graph dataset, including the famous "Zachary" karate club network, yes, the one "*If you cannot get it right on this network, then go home*". 

All these features make it a perfect tool in network science research.

```{r,message=FALSE, warning=FALSE}
library('igraph')
karate <- make_graph("Zachary")
plot(karate)
```

Here we explicitly implement the idea of computing the leading eigenvector of modularity matrix $B$.
```{r,message=FALSE, warning=FALSE}
A <- as_adjacency_matrix(karate)
A <- as.matrix(A)
k <- colSums(A)
m <- sum(k)/2
Km <- k%*%t(k)
B <- A-Km/(2*m)
r <- eigen(B)
u <- r$vectors[,1]
wcr <- list(which(u>0),which(u<0))
plot(karate,mark.groups=wcr)
```

Note that the overlapped color does not mean the two communities are overlapping.

To compute modularity Q:

```{r,message=FALSE, warning=FALSE}
s <- u
s[which(s<0)] <- -1
s[which(s>0)] <- 1
print(Q <- sum(B*(s%*%t(s)+1))/(4*m))
```
Or using the generalized eigenvector equation in PRE13, we have

```{r,message=FALSE, warning=FALSE}
D <- diag(sqrt(k))
Dp <- solve(D)
L <- Dp%*%A%*%Dp
r <- eigen(L)
u <- Dp%*%r$vectors[,2]
wcr <- list(which(u>0),which(u<0))
plot(karate,mark.groups=wcr)
```

Calling community detection algorithms in igraph is straightforward. Function `cluster_leading_eigen` implements the spectral algorithm in PRE06, and function `modularity` directly gives out the Q value for a partition. The implementation considers further division thus returns four communities, and the Q value is higher.

```{r,eval=FALSE}
wc <- cluster_leading_eigen(karate)
membership(wc)
modularity(wc)
plot(wc, karate)
```

--
Last update: `r Sys.setlocale("LC_TIME", "English"); format(Sys.Date(), "%B %d, %Y")`