---
title: "Why it converges: gradient descent"
author: "Dong Li"
date: "15 August 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation

> A method of writing proofs is proposed that makes it much harder to prove
things that are not true. -- Leslie Lamport. "How to write a proof." 1993

Gradient descend is probably the most intuitive optimization algorithm. Just like the other methods, first we use it for granted, then we try to figure out why it works. The most important part of this reasoning is to prove it rigorously, which has been completed by mathematicians long time ago, and been  repeated again and again by many people. But it is still necessary to write it down myself, to enhance my understanding.

# Algorithm

Given a convex and differentiable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$, we want to minimize it, 
$$min_{{\bf x}\in\mathbb{R}^n} f({\bf x})$$
We assume the optimal solution exists, i.e. $f({\bf x}^*)=minf({\bf x})$.

The gradient descend: start with an initial ${\bf x}^{(0)}\in\mathbb{R}^n$, repeat
$${\bf x}^{(k)}={\bf x}^{(k-1)}-t_k\nabla f({\bf x}^{(k-1)}),\ k=1,2,3...$$
until some stopping criteria is satisified. $t_k$ is the step size at $k$-th iteration, which can be picked as fixed or flexible.

# A simple example
Image we want to minimize $f(x)=x^2$ in one-dimensional space, $\nabla f(x)=2x$, we set step size $t=0.01$, and maximal iteration $T=1000$
```{r,message=FALSE, warning=FALSE}
x0 <- runif(1,0,1)
T <- 1000
t <- 0.01
x <- x0
tao <- 1e-9
obj <- numeric(length=T)
for (i in 1:T){
    obj[i] <- x^2
    xnew <- x-t*2*x
    if (abs(xnew-x)<tao){
        break
    }
    x <- xnew
}
obj <- obj[1:i]
plot(obj,xlab='Iteration',ylab='Objective')
```

# Convergence analysis
We start with the convexity of $f$:
$$
f({\bf y})\geq f({\bf x})+\nabla f({\bf x})^\top({\bf y}-{\bf x})
$$
which can be derived from the Jensen's inequality, or a straightforward geometric interpretation.

We define Lipschitz continuity:

**Definition**: A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called Lipschitz continuous if there exists a positive real constant $K$ such that
$$
\|f({\bf x})-f({\bf y})\|_2\leq K\|{\bf x}-{\bf y}\|_2,\ \forall {\bf x},{\bf y} \in \mathbb{R}^n.
$$

Here we need to assume the *gradient* of $f$ is Lipschitz continuous with constant $L$, i.e.
$$
\|\nabla f({\bf x})-\nabla f({\bf y})\|_2\leq L\|{\bf x}-{\bf y}\|_2,\ \forall {\bf x},{\bf y} \in \mathbb{R}^n.
$$
We can also say $\nabla f({\bf x})$ is L-Lipschitz in this case.

**Lemma** The Lipschitz continuity of $\nabla f({\bf x})$ leads to
$$
f({\bf y})\leq f({\bf x}) + \nabla f({\bf x})^\top({\bf y}-{\bf x})+ \frac{L}{2}\|{\bf x}-{\bf y}\|^2,\ \forall {\bf x},{\bf y}
$$
**proof**
We construct an auxiliary function $g(t)=f({\bf x}+t({\bf y}-{\bf x}))$, note $g(1)=f({\bf y})$ and $g(0)=f({\bf x})$. Furthermore,
$$
\nabla g(t)=\nabla f({\bf x}+t({\bf y}-{\bf x}))^\top({\bf y}-{\bf x})\\
\Rightarrow \int_0^1 \nabla g(t)dt=g(1)-g(0)=f({\bf y})-f({\bf x})
$$
We have
$$\begin{aligned}
&f({\bf y})- f({\bf x}) - \nabla f({\bf x})^\top({\bf y}-{\bf x})\\
&=\int_0^1 \nabla f({\bf x}+t({\bf y}-{\bf x}))^\top({\bf y}-{\bf x})dt- \nabla f({\bf x})^\top({\bf y}-{\bf x})\\
&=({\bf y}-{\bf x})^\top(\int_0^1 \nabla f({\bf x}+t({\bf y}-{\bf x}))dt-\nabla f({\bf x}))\\
&=({\bf y}-{\bf x})^\top\int_0^1 (\nabla f({\bf x}+t({\bf y}-{\bf x}))-\nabla f({\bf x}))dt\\
&\leq ({\bf y}-{\bf x})^\top\int_0^1 \|\nabla f({\bf x}+t({\bf y}-{\bf x}))-\nabla f({\bf x})\|dt\\
&\leq ({\bf y}-{\bf x})^\top\int_0^1 \frac{L}{2}\|t({\bf y}-{\bf x})\|dt\\
&=\frac{L}{2}\|{\bf y}-{\bf x}\|^2
\end{aligned}$$
Note we use Cauchy Schwartz inequality in line 4-5, and the definition of Lipschitz continuity of $\nabla f({\bf x})$ in line 5-6.
Q.E.D

**Theorem**: Gradient descend with a fixed step-size $t\leq 1/L$ satisifies
$$
f({\bf x}^{(k)})-f({\bf x}^*)\leq \frac{\|{\bf x}^{(0)}-{\bf x}^*\|}{2tk}
$$

This theorem is the main theoretical result of gradient descend, which guarantees the algorithm has convergence rate $O(1/k)$. In other words, the algorithm guarantees to approximate the $\epsilon$-accuracy, i.e. $f({\bf x}^{(k)})-f({\bf x}^*)\leq \epsilon$ in $O(1/\epsilon)$ iterations.

**Proof**:
Assume ${\bf y}={\bf x}-t\nabla f({\bf x})$ of above lemma, and a fixed step size $0<t\leq 1/L$ and $0< t\leq 1/L$,
$$
f({\bf y})\leq f({\bf x})-t\|\nabla f({\bf x})\|^2+\frac{Lt^2}{2}\|\nabla f({\bf x})\|^2\\
=f({\bf x})-t(1-\frac{Lt}{2})\|\nabla f({\bf x})\|^2
$$
Denote ${\bf x}^+={\bf x}-t\nabla f({\bf x})$, and note $Lt\leq 1$, we have
$$\begin{aligned}
&f({\bf x}^+)\leq f({\bf x})-\frac{t}{2}\|\nabla f({\bf x})\|^2\\
&\leq f^*+f({\bf x})^\top({\bf x}-{\bf x}^*)-\frac{t}{2}\|\nabla f({\bf x})\|^2\\
&=f^*+\frac{1}{2t}(\|{\bf x}-{\bf x}^*\|^2-\|{\bf x}-{\bf x}^*-t\nabla f({\bf x})\|^2)\\
&=f^*+\frac{1}{2t}(\|{\bf x}-{\bf x}^*\|^2-\|{\bf x}^+-{\bf x}^*\|^2)
\end{aligned}$$
Note we apply the convexity inequality in 1-2 line.

We now define ${\bf x}={\bf x}^{(i-1)}$ and ${\bf x}^+={\bf x}^{(i)}$, and sum over iterations:
$$\begin{aligned}
&\sum_{i=1}^k(f({\bf x}^{(i)})-f^*)\leq \frac{1}{2t}\sum_{i=1}^k(\|{\bf x}^{(i-1)}-{\bf x}^*\|^2-\|{\bf x}^{(i)}-{\bf x}^*\|^2)\\
&=\frac{1}{2t}(\|{\bf x}^{(0)}-{\bf x}^*\|^2-\|{\bf x}^{(i)}-{\bf x}^*\|^2)\\
&\leq \frac{1}{2t}(\|{\bf x}^{(0)}-{\bf x}^*\|^2)
\end{aligned}$$

Since $f({\bf x}^{(k)})$ is non-increasing, 
$$
f({\bf x}^{(k)})-f^* \leq
\frac{1}{k}\sum_{i=1}^k(f({\bf x}^{(i)})-f^*)
\leq \frac{1}{2tk}(\|{\bf x}^{(0)}-{\bf x}^*\|^2)
$$
Q.E.D

# Reference

Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

L. Vandenberghe's Lecture notes: http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf

Strongly Convex: http://www.stronglyconvex.com/blog/gradient-descent.html

Geoffrey J. Gordon. Gradient descent revisited: https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf

Marco Tulio Ribeiro's Blog: https://homes.cs.washington.edu/~marcotcr/blog/gradient-descent/
