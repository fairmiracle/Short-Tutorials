---
title: "A Brief Note on First-order Methods"
author: "Dong Li"
date: "15 August 2017"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation

> A method of writing proofs is proposed that makes it much harder to prove
things that are not true. -- Leslie Lamport. "How to write a proof." 1993

Gradient descent is probably the most intuitive optimization algorithm. It is also the foundation of the first-order optimization algorithms. Just like the other methods, first we use it for granted, then we try to figure out why it works. The most important part of this reasoning is to prove it rigorously, which has been completed by mathematicians long time ago, and been repeated again and again by many people. But it is still necessary to write it down myself, to enhance my understanding.

# Basic gradient descent

Given a convex and differentiable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$, we want to minimize it, 
$$min_{x\in\mathbb{R}^n} f(x)$$
We assume the optimal solution exists, i.e. $f(x^*)=minf(x)$.

The gradient descent: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat
$$x^{(k)}=x^{(k-1)}-t_k\nabla f(x^{(k-1)}),\ k=1,2,3...$$
until some stopping criteria is satisified. $t_k$ is the step size at $k$-th iteration, which can be picked as fixed or flexible.

## A simple example
Image we want to minimize $f(x)=x^2$ in one-dimensional space, $\nabla f(x)=2x$, we set step size $t=0.01$, and maximal iteration $T=1000$
```{r,message=FALSE, warning=FALSE}
x0 <- runif(1,0,1)
T <- 1000
t <- 0.01
x <- x0
tao <- 1e-9
obj <- numeric(length=T)
for (i in 1:T){
    obj[i] <- x^2
    xnew <- x-t*2*x
    if (abs(xnew-x)<tao){
        break
    }
    x <- xnew
}
obj <- obj[1:i]
plot(obj,xlab='Iteration',ylab='Objective')
```

## Convergence analysis
We start with the convexity of $f$:
$$
f(y)\geq f(x)+\nabla f(x)^\top(y-x)
$$
which can be derived from the Jensen's inequality, or a straightforward geometric interpretation.

We define Lipschitz continuity:

**Definition**: A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called Lipschitz continuous if there exists a positive real constant $K$ such that
$$
\|f(x)-f(y)\|_2\leq K\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
$$

Here we need to assume the *gradient* of $f$ is Lipschitz continuous with constant $L$, i.e.
$$
\|\nabla f(x)-\nabla f(y)\|_2\leq L\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
$$
We can also say $\nabla f(x)$ is L-Lipschitz in this case, or $f$ is L-smooth.

**Lemma** The Lipschitz continuity of $\nabla f(x)$ leads to
$$
f(y)\leq f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2,\ \forall x,y
$$
**proof**
We construct an auxiliary function $g(t)=f(x+t(y-x))$, note $g(1)=f(y)$ and $g(0)=f(x)$. Furthermore,
$$
\nabla g(t)=\nabla f(x+t(y-x))^\top(y-x)\\
\Rightarrow \int_0^1 \nabla g(t)dt=g(1)-g(0)=f(y)-f(x)
$$
We have
$$\begin{aligned}
&f(y)- f(x) - \nabla f(x)^\top(y-x)\\
&=\int_0^1 \nabla f(x+t(y-x))^\top(y-x)dt- \nabla f(x)^\top(y-x)\\
&=(y-x)^\top(\int_0^1 \nabla f(x+t(y-x))dt-\nabla f(x))\\
&=(y-x)^\top\int_0^1 (\nabla f(x+t(y-x))-\nabla f(x))dt\\
&\leq (y-x)^\top\int_0^1 \|\nabla f(x+t(y-x))-\nabla f(x)\|dt\\
&\leq (y-x)^\top\int_0^1 \frac{L}{2}\|t(y-x)\|dt\\
&=\frac{L}{2}\|y-x\|^2
\end{aligned}$$
Note we use Cauchy Schwartz inequality in line 4-5, and the definition of Lipschitz continuity of $\nabla f(x)$ in line 5-6.

Q.E.D.

Another **proof**: Since $f$ is convex, we can combine $f(y)\geq f(x)+\nabla f(x)^\top(y-x)$ and $f(x)\geq f(y)+\nabla f(y)^\top(x-y)$ to get 
$$
(\nabla f(x)-\nabla f(y))^\top (x-y) \geq 0
$$
We then construct $g(x)=\frac{L}{2}x^\top x-f(x)$.
$$\begin{aligned}
&\nabla g(x)-\nabla g(y) = L(x-y)-(\nabla f(x)-\nabla f(y))\\
& \Rightarrow (\nabla g(x)-\nabla g(y))^\top (x-y)\\
&=L(x-y)^\top(x-y)-(\nabla f(x)-\nabla f(y))^\top (x-y) \geq 0
\end{aligned}$$
we can claim $g(x)$ is convex, which means 
$$\begin{aligned}
& g(y) \geq g(x) + \nabla g(x)^\top (y-x)\\
& \Rightarrow \frac{L}{2} y^\top y-f(y) \geq \frac{L}{2} x^\top x-f(x)+(Lx-\nabla f(x))^\top (y-x)\\
& \Rightarrow f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2 \geq f(y)
\end{aligned}$$

Q.E.D.

A third **proof**: According to $\nabla f$ is Lipschitz continuous with constant $L$, $\|\nabla f(x)-\nabla f(y)\|_2\leq L\|x-y\|_2$, let $x=y+\Delta$ where $\Delta \rightarrow 0$, we have $\nabla^2 f \preceq LI$.

For any $z \in dom f$,
$$\begin{aligned}
& \nabla^2 f(z) \preceq LI\\
& \Rightarrow (x-y)^\top (\nabla^2 f(z)-LI) (x-y)\leq 0\\
& \Rightarrow (x-y)^\top \nabla^2 f(z) (x-y)\leq L \|x-y\|^2
\end{aligned}$$

We can use Taylor's Remainder Theorem:
$$\begin{aligned}
&f(y)= f(x) + \nabla f(x)^\top(y-x)+ \frac{1}{2}(x-y)^\top \nabla^2 f(x)(x-y) \\
&\leq f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2
\end{aligned}$$

Q.E.D.

Note that the second and third proof are identical since when a convex function $f$ is twice differentiable, its Hessian is positive semidefite:
$$
\nabla^2 f(x) \succeq 0.
$$
Therefore, $\nabla^2 g \succeq 0 \rightarrow \nabla^2 f \preceq LI$.

**Theorem**: Gradient descent with a fixed step-size $t\leq 1/L$ satisifies
$$
f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}
$$

This theorem is the main theoretical result of gradient descent, which guarantees the algorithm has convergence rate $O(1/k)$. In other words, the algorithm guarantees to approximate the $\epsilon$-accuracy, i.e. $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\epsilon)$ iterations.

**Proof**:
Assume $y=x-t\nabla f(x)$ of above lemma, and a fixed step size $0<t\leq 1/L$,
$$
f(y)\leq f(x)-t\|\nabla f(x)\|^2+\frac{Lt^2}{2}\|\nabla f(x)\|^2\\
=f(x)-t(1-\frac{Lt}{2})\|\nabla f(x)\|^2
$$
Denote $x^+=x-t\nabla f(x)$, and note $Lt\leq 1$, we have
$$\begin{aligned}
&f(x^+)\leq f(x)-\frac{t}{2}\|\nabla f(x)\|^2\\
&\leq f^*+f(x)^\top(x-x^*)-\frac{t}{2}\|\nabla f(x)\|^2\\
&=f^*+\frac{1}{2t}(\|x-x^*\|^2-\|x-x^*-t\nabla f(x)\|^2)\\
&=f^*+\frac{1}{2t}(\|x-x^*\|^2-\|x^+-x^*\|^2)
\end{aligned}$$
Note we apply the convexity inequality in 1-2 line.

We now define $x=x^{(i-1)}$ and $x^+=x^{(i)}$, and sum over iterations:
$$\begin{aligned}
&\sum_{i=1}^k(f(x^{(i)})-f^*)\leq \frac{1}{2t}\sum_{i=1}^k(\|x^{(i-1)}-x^*\|^2-\|x^{(i)}-x^*\|^2)\\
&=\frac{1}{2t}(\|x^{(0)}-x^*\|^2-\|x^{(i)}-x^*\|^2)\\
&\leq \frac{1}{2t}(\|x^{(0)}-x^*\|^2)
\end{aligned}$$

Since $f(x^{(k)})$ is non-increasing, 
$$
f(x^{(k)})-f^* \leq
\frac{1}{k}\sum_{i=1}^k(f(x^{(i)})-f^*)
\leq \frac{1}{2tk}(\|x^{(0)}-x^*\|^2)
$$

Q.E.D.

# Subgradient method
Sometimes a function is not differentiable at certain points, such as $\|x\|_1$ at 0. We still want to use gradient based approaches. Being similar to the differentiable convex functions, we have

**Definition**: A subgradient of a convex function $f$ is $g\in \mathbb{R}^n$ such that
$$
f(y)\geq f(x)+g^\top(y-x),\  \forall x,y \in \mathbb{R}^n.
$$
**Definition**: A subdifferential is a closed convex set of all subgradients of a convex function $f$:
$$
\partial f(x)=\{g\in \mathbb{R}^n: g \textrm{ is a subgradient of }f \textrm{ at } x\}
$$

The subgradient descent: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat
$$x^{(k)}=x^{(k-1)}-t_k g^{(k-1)},\ k=1,2,3...$$
until some stopping criteria is satisified. Where $g^{(k-1)}$ is any subgradient of $f$ at $x^{(k-1)}$. $t_k$ is the step size at $k$-th iteration, which can be picked as fixed or flexible.

## Convergence analysis
We assume the convex function $f$ satistifes:

- $f$ is Lipschitz continuous with constant $L$.
  $$
  \|f(x)-f(y)\|_2\leq L\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
  $$
- $\|x^{(1)}-x^*\|_2\leq R$ which means it is bounded.

**Theorem**: Subgradient gradient descent with a fixed step-size $t$ satisifies
$$
\lim_{k\to\infty} f(x^{(k)}_{best})\leq f(x^*)+\frac{L^2t}{2}
$$


**Proof**: 
$$\begin{aligned}
\|x^{(k+1)}-x^*\|^2&=\|x^{(k)}-t_k g^{(k)}-x^*\|^2\\
&=\|x^{(k)}-x^*\|^2-2t_k g^{(k)\top} (x^{(k)}-x^*)+t_k^2 \|g^{(k)}\|^2
\end{aligned}$$

By the definition of subgradient we have
$$\begin{aligned}
&f(x^*)\geq f(x)+g^\top(x^*-x)\\
&\Rightarrow -g^{(k)\top} (x^{(k)}-x^*) \leq f(x^*)- f(x^{(k)})
\end{aligned}$$

we now have
$$\begin{aligned}
\|x^{(k+1)}-x^*\|^2 & \leq \|x^{(k)}-x^*\|^2+2t_k(f(x^*)- f(x^{(k)}))+t_k^2 \|g^{(k)}\|^2\\
& \leq \|x^{(1)}-x^*\|^2+2\sum_{i=1}^kt_i(f(x^*)- f(x^{(i)}))+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
& \leq R^2+2\sum_{i=1}^kt_i(f(x^*)- f(x^{(i)}))+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
\end{aligned}$$
Since $\|x^{(k+1)}-x^*\|^2 \geq 0$
$$\begin{aligned}
&2\sum_{i=1}^kt_i(f(x^{(i)})-f(x^*))\leq R^2+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
\Rightarrow & 2(\sum_{i=1}^kt_i)(f(x^{(k)}_{best})-f(x^*))\leq R^2+\sum_{i=1}^kt_i^2 L^2
\end{aligned}$$

- For a fixed step size:
$$
\lim_{k\to \infty} \frac{R^2+kt^2 L^2}{2tk}=\frac{L^2t}{2}
$$
- For a specific step size: $t_i=R/(L\sqrt{k})$
$$
\frac{R^2+\sum_{i=1}^kt_i^2 L^2}{2(\sum_{i=1}^kt_i)}=\frac{RL}{\sqrt{k}}
$$

The algorithm has convergence rate $O(1/\sqrt{k})$ or get $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\epsilon^2)$ iterations.

Q.E.D.

# Proximal gradient descent
Before we start, refer to the third proof of Lemma in gradient descent, we use the quadratic approximation for a convex function $f$ at certain point. 
$$\begin{aligned}
f(x) + \nabla f(x)^\top(z-x)+ \frac{1}{2}(z-x)^\top \nabla^2 f(x)(z-x)
\end{aligned}$$

By replacing the $\nabla^2 g$ by $I/t$ we can derive the radient update rule (with fixed step size $t$)
$$\begin{aligned}
& x^+=\arg\min_z f(x) + \nabla f(x)^\top(z-x)+ \frac{1}{2t}\|z-x\|^2\\
& \Rightarrow x^+= x-t \nabla f(x)
\end{aligned}$$

The gradient descent can be generalized to work for a class of general functions:
$$
f(x)=g(x)+h(x)
$$
where $g(x)$ is convex and differentiable while $h(x)$ is convex but not differentiable. It is common in machine learning since a dozen of classical models can be expressed as "loss+regularization" when the regularization may not be differentiable everywhere (such as $\ell_1$-norm).

Here we can do the same quadratic approximation for differentiable part $g(x)$, but keep the other part $h(x)$ unchanged for now, the update rule is
$$\begin{aligned}
&x^+ =\arg\min_z g(x) + \nabla g(x)^\top(z-x)+ \frac{1}{2t}\|z-x\|^2+h(z)\\
& = \arg\min_z \frac{1}{2t}\|z-(x-t\nabla g(x))\|^2+h(z)
\end{aligned}$$

Note the operator 
$$\begin{aligned}
{\rm prox}_h(x)=\arg\min_z \frac{1}{2}\|z-x\|^2+h(z)
\end{aligned}$$
is also called proximal mapping or prox-operator of convex function $h$. Examples of prox-operator include:

 - $h=0$, it becomes normal **gradient descent**.
 
 - $h$ is indicator function of closed convex set $C$, where
 $$\begin{aligned}
h(x)=I_C(x)=\left\{
  \begin{array}{ll}
    0 & x\in C\\
    \infty & x\notin C
  \end{array}
\right.
\end{aligned}$$

 Then ${\rm prox}_h$ is a projection on $C$:
 $$\begin{aligned}
 {\rm prox}_h(x)=P_C(x)=\arg\min_{z\in C}\|z-x\|^2
 \end{aligned}$$
 It is also called **projected gradient descent**.
 
 - $h(x)=\|x\|_1$, ${\rm prox}_h$ is the soft-threshold operator:
$$\begin{aligned}
{\rm prox}_h(x)_i=\left\{
  \begin{array}{ll}
    x_i-1 & x_i\geq 1\\
    0 & |x_i|< 1\\
    x_i+1 & x_i\leq -1
  \end{array}
\right.
\end{aligned}$$

It is also called Iterative Soft-Thresholding Algorithm (**ISTA**).

The proximal gradient descent algorithm: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat

$$\begin{aligned}
& x^+=x^{(k-1)}-t_k\nabla f(x^{(k-1)})\\
& x^{(k)}= {\rm prox}_h(x^+)
\end{aligned}$$
until some stopping criteria is satisified.

The updating rule can be rewritten in a similar way to gradient descent:
$$\begin{aligned}
x^{(k)}=x^{(k-1)}-t_kG_{t_k}(x^{(k-1)})
\end{aligned}$$
where
$$\begin{aligned}
G_t(x)=\frac{x-{\rm prox}_h(x-t\nabla g(x))}{t}
\end{aligned}$$

## Convergence analysis
Assume $g$ has the same properties as in the previous section, and $h$ is convex and ${\rm prox}_h$ is cheap to calculate, then we have

**Lemma** The Lipschitz continuity of $\nabla g(x)$ leads to
$$
f(y)\leq g(x) + \nabla g(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2+h(y),\ \forall x,y
$$
The proof keeps the same.

**Theorem**: Generalized gradient descent with a fixed step-size $t\leq 1/L$ satisifies

$$
f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}
$$

It has the same convergence rate $O(1/k)$ as gradient descent.

**Proof**:
Assume $y=x^+=x-tG_t(x)$ of above lemma, and a fixed step size $0<t\leq 1/L$,
$$
f(x^+)\leq g(x)-t\nabla g(x)^\top G_t(x)+\frac{Lt^2}{2}\|G_t(x)\|^2+h(x-tG_t(x))
$$
Note in the updating rule
$$\begin{aligned}
x^+ = x-tG_t(x) &=\arg\min_z \frac{1}{2t}\|z-(x-t\nabla g(x))\|^2+h(z)\\
& = \arg\min_z \nabla g(x)^\top (z-x)+\frac{1}{2t}\|z-x\|^2+h(z)
\end{aligned}$$

Since $h(z)$ is not differentiable, the optimality condition is
$$
\nabla g(x)+\frac{1}{t}(z-x)+v=0
$$
where $v\in \partial h(z)$. Also note $z=x-tG_t(x)$. We get
$$\begin{aligned}
& \nabla g(x)-G_t(x)+v=0\\
& \Rightarrow G_t(x)-g(x) \in \partial h(x-tG_t(x))
\end{aligned}$$
Since $h$ is convex
$$\begin{aligned}
& h(x)\geq h(x-tG_t(x))+(G_t(x)-g(x))^\top tG_t(x)\\
& \Rightarrow h(x-tG_t(x)) \leq h(x)-t(G_t(x)-g(x))^\top G_t(x)
\end{aligned}$$

We carry on the beginning of this proof,
$$\begin{aligned}
&f(x^+)\leq g(x)-t\nabla g(x)^\top G_t(x)+\frac{Lt^2}{2}\|G_t(x)\|^2+h(x)-t(G_t(x)-g(x))^\top G_t(x)\\
& = f(x)+t(1-\frac{Lt}{2})\|G_t(x)\|^2\\
&\leq f(x)-\frac{t}{2}\|G_t(x)\|^2\\
\end{aligned}$$
We should stop here because everything keeps the same as in the proof of gradient descent, except that we replace $\nabla f(x)$ with $G_t(x)$.

Q.E.D.


# Acceleration

The generalized gradient descent (of course including the simple gradient descent) can be accelerated by using Nesterov's method. The idea is to use a linear combination of previous two steps instead of one, to compute a candidate solution.

The algorithm: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat

$$\begin{aligned}
& v=x^{(k-1)}+\frac{k-2}{k+1}(x^{(k-1)}-x^{(k-2)})\\
& x^{(k)}= {\rm prox}_h(v-t_k\nabla g(v))
\end{aligned}$$
until some stopping criteria is satisified.

## Convergence analysis

**Theorem**: Accelerated generalized gradient descent with a fixed step-size $t\leq 1/L$ satisifies
$$
f(x^{(k)})-f(x^*)\leq \frac{2\|x^{(0)}-x^*\|^2}{t(k+1)^2}
$$

It has the convergence rate $O(1/k^2)$, or it gets $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\sqrt{\epsilon})$ iterations.

**Proof**: Since $t\leq 1/L$ we have
$$\begin{aligned}
& g(x^+)\leq g(y) + \nabla g(y)^\top (x^+-y)+ \frac{L}{2}\|x^+-y\|^2\\
& \leq g(y) + \nabla g(y)^\top (x^+-y)+ \frac{1}{2t}\|x^+-y\|^2
\end{aligned}$$

Suppose we have 
$$\begin{aligned}
&v = {\rm prox}_{h,t}(x)=\arg\min_z \frac{1}{2t}\|x-v\|^2+h(v)\\
&\Rightarrow 0 \in \partial ( \frac{1}{2t}\|x-v\|^2+h(v))= -\frac{1}{t}(x-v)+ \partial h(v)\\
&\Rightarrow -\frac{1}{t}(x-v)\in \partial h(v)\\
&\Rightarrow h(z) \geq h(v) - \frac{1}{t}(x-v)^\top (z-v)\, \forall z\\
&\Rightarrow h(v) \leq h(z) + \frac{1}{t}(x-v)^\top (z-v)\, \forall z
\end{aligned}$$

Note that we use the definition of subgradient in Line 3-4.

Now we use $x^+ = {\rm prox}_{h,t}(y-t\nabla g(y))$, 
$$\begin{aligned}
&h(x^+) \leq h(z) + \frac{1}{t}(x^+-y+t\nabla g(y))^\top (z-x^+)\, \forall z\\
& = h(z) + \frac{1}{t}(x^+-y)^\top (z-x^+) + \nabla g(y)^\top (z-x^+)\, \forall z
\end{aligned}$$

Combing $h(x^+)$ and $g(x^+)$ we have
$$\begin{aligned}
&f(x^+) \leq  g(y) + h(z)+ \frac{1}{2t}\|x^+-y\|^2+ \frac{1}{t}(x^+-y)^\top (z-x^+) + \nabla g(y)^\top (z-y)\\
& \leq  h(z)+ g(z)+\frac{1}{2t}\|x^+-y\|^2+\frac{1}{t}(x^+-y)^\top (z-x^+)\\
& = f(z)+\frac{1}{2t}\|x^+-y\|^2+\frac{1}{t}(x^+-y)^\top (z-x^+)
\end{aligned}$$
Note we use $g(y)+ \nabla g(y)^\top (z-y) \leq  g(z)$ because of convexity of $g$ in line 1-2.

The following constructions are quite tricky. 

Let $z=x$ and $z=x^*$, multiply both sides by $1-\theta$ and $\theta$ respectively,

$$\begin{aligned}
&(1-\theta) f(x^+) \leq (1-\theta) f(x)+\frac{1-\theta}{2t}\|x^+-y\|^2+\frac{1-\theta}{t}(x^+-y)^\top (x-x^+)\\
&\theta f(x^+) \leq \theta f(x^*)+\frac{\theta}{2t}\|x^+-y\|^2+\frac{\theta}{t}(x^+-y)^\top (x^*-x^+)\\
&f(x^+) \leq (1-\theta)f(x)+\theta f(x^*)+\frac{1}{t}(x^+-y)^\top ((1-\theta)x-x^+ + \theta x^*)+\frac{1}{2t}\|x^+-y\|^2\\
& = f(x^*) + (1-\theta)(f(x)-f(x^*)) +\frac{1}{t}(x^+-y)^\top ((1-\theta)x-x^+ + \theta x^*)+\frac{1}{2t}\|x^+-y\|^2
\end{aligned}$$

Using $u^+=x+\frac{1}{\theta}(x^+-x)$ and $y=(1-\theta)x+\theta u$, we have
$(1-\theta)x+\theta x^*-x^+=\theta(x^*-u^*)$ and $x^+-y=\theta(u^+-u)$, substitute these equations we have
$$\begin{aligned}
\begin{array}{ll}
f(x^+)-f(x^*)-(1-\theta)(f(x)-f(x^*))&\leq \dfrac{\theta}{2t}(u^+-u)^\top [2\theta(x^*-u^+)+\theta(u^+-u)]\\[8pt]
&=\dfrac{\theta^2}{2t}[(x^*-u)-(x^*-u^+)]^\top[(x^*-u^+)+(x^*-u)]\\[8pt]
&=\dfrac{\theta^2}{2t}(||x^*-u||^2-||x^*-u^+||^2)\\
\end{array}
\end{aligned}$$

Back to $k$ notation we have:
$$\begin{aligned}
\dfrac{t}{\theta_k^2}(f(x^{(k)})-f(x^*))+\dfrac{1}{2}||u^{(k)}-x^*||^2
\leq \dfrac{t(1-\theta_k)}{\theta_k^2}(f(x^{(k-1)})-f(x^*))+\dfrac{1}{2}||u^{(k-1)}-x^*||^2
\end{aligned}$$

Using $\frac{1-\theta_k}{\theta^2_k}\leq \frac{1}{\theta^2_{k-1}}$ we have
$$\begin{aligned}
\dfrac{t}{\theta_k^2}(f(x^{(k)})-f(x^*))+\dfrac{1}{2}||u^{(k)}-x^*||^2
\leq \dfrac{t}{\theta_{k-1}^2}(f(x^{(k-1)})-f(x^*))+\dfrac{1}{2}||u^{(k-1)}-x^*||^2
\end{aligned}$$

Iterate this inequality and use $\theta_1=1, u^{(0)}=x^{(0)}$ we get
$$\begin{aligned}
\dfrac{t}{\theta_k^2}(f(x^{(k)})-f(x^*))+\dfrac{1}{2}||u^{(k)}-x^*||^2
\leq \dfrac{t(1-\theta_1)}{\theta_1^2}(f(x^{(0)})-f(x^*))+\dfrac{1}{2}||u^{(0)}-x^*||^2=\dfrac{1}{2}||x^{(0)}-x^*||^2
\end{aligned}$$

Hence we conclude
$$\begin{aligned}
f(x^{(k)})-f(x^*)\leq \dfrac{\theta_k^2}{2t}||x^{(0)}-x^*||^2 = \dfrac{2||x^{(0)}-x^*||^2}{t(k+1)^2}
\end{aligned}$$

Q.E.D.

# Endnote
This post is about the first-order methods and their convergence analysis. Compare these algorithms regarding assumption and covergence rate:

|  Algorithm   | Function  | Condition | Convergence rate   |
|-------------+--------------+------------------------+-----------|
| Gradient descent  | $f(x)$   | $f$: convex, differentiable, L-smooth  | $O(1/k)$ |
| Subgradient descent | $f(x)=g(x)+h(x)$ | $g$: convex, differentiable, L-smooth; $h$:convex  | $O(1/\sqrt{k})$ |
| Generalized gradient descent | $f(x)=g(x)+h(x)$ | $g$: convex, differentiable, L-smooth; $h$:convex; ${\rm prox}_h(x)$ is cheap   | $O(1/k)$ |
| Accelerated gradient descent   | $f(x)=g(x)+h(x)$   | $g$: convex, differentiable, L-smooth; $h$:convex; ${\rm prox}_h(x)$ is cheap | $O(1/k^2)$ |

Table: A summary of first-order methods.

Acceleration seems work perfectly, but not all problems are suitable to be accelerated. Even subgradient method has slower convergence rate, it works when the proximal operator ${\rm prox}_h(x)$ is expensive.

# Reference

Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

Lieven Vandenberghe: http://www.seas.ucla.edu/~vandenbe/236C

Strongly Convex: http://www.stronglyconvex.com/blog/gradient-descent.html

Geoff Gordon and Ryan Tibshirani. https://www.cs.cmu.edu/~ggordon/10725-F12

Marco Tulio Ribeiro's Blog: https://homes.cs.washington.edu/~marcotcr/blog/gradient-descent

<!-- Sebastien Bubeck's Blog: https://blogs.princeton.edu/imabandit/archives -->

--
Last update: `r Sys.setlocale("LC_TIME", "English"); format(Sys.Date(), "%B %d, %Y")`
