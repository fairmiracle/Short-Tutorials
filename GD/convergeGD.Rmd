---
title: "Why it converges: gradient descent"
author: "Dong Li"
date: "15 August 2017"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation

> A method of writing proofs is proposed that makes it much harder to prove
things that are not true. -- Leslie Lamport. "How to write a proof." 1993

Gradient descent is probably the most intuitive optimization algorithm. Just like the other methods, first we use it for granted, then we try to figure out why it works. The most important part of this reasoning is to prove it rigorously, which has been completed by mathematicians long time ago, and been  repeated again and again by many people. But it is still necessary to write it down myself, to enhance my understanding.

# Basic gradient descent

Given a convex and differentiable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$, we want to minimize it, 
$$min_{x\in\mathbb{R}^n} f(x)$$
We assume the optimal solution exists, i.e. $f(x^*)=minf(x)$.

The gradient descent: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat
$$x^{(k)}=x^{(k-1)}-t_k\nabla f(x^{(k-1)}),\ k=1,2,3...$$
until some stopping criteria is satisified. $t_k$ is the step size at $k$-th iteration, which can be picked as fixed or flexible.

## A simple example
Image we want to minimize $f(x)=x^2$ in one-dimensional space, $\nabla f(x)=2x$, we set step size $t=0.01$, and maximal iteration $T=1000$
```{r,message=FALSE, warning=FALSE}
x0 <- runif(1,0,1)
T <- 1000
t <- 0.01
x <- x0
tao <- 1e-9
obj <- numeric(length=T)
for (i in 1:T){
    obj[i] <- x^2
    xnew <- x-t*2*x
    if (abs(xnew-x)<tao){
        break
    }
    x <- xnew
}
obj <- obj[1:i]
plot(obj,xlab='Iteration',ylab='Objective')
```

## Convergence analysis
We start with the convexity of $f$:
$$
f(y)\geq f(x)+\nabla f(x)^\top(y-x)
$$
which can be derived from the Jensen's inequality, or a straightforward geometric interpretation.

We define Lipschitz continuity:

**Definition**: A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is called Lipschitz continuous if there exists a positive real constant $K$ such that
$$
\|f(x)-f(y)\|_2\leq K\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
$$

Here we need to assume the *gradient* of $f$ is Lipschitz continuous with constant $L$, i.e.
$$
\|\nabla f(x)-\nabla f(y)\|_2\leq L\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
$$
We can also say $\nabla f(x)$ is L-Lipschitz in this case, or $f$ is L-smooth.

**Lemma** The Lipschitz continuity of $\nabla f(x)$ leads to
$$
f(y)\leq f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2,\ \forall x,y
$$
**proof**
We construct an auxiliary function $g(t)=f(x+t(y-x))$, note $g(1)=f(y)$ and $g(0)=f(x)$. Furthermore,
$$
\nabla g(t)=\nabla f(x+t(y-x))^\top(y-x)\\
\Rightarrow \int_0^1 \nabla g(t)dt=g(1)-g(0)=f(y)-f(x)
$$
We have
$$\begin{aligned}
&f(y)- f(x) - \nabla f(x)^\top(y-x)\\
&=\int_0^1 \nabla f(x+t(y-x))^\top(y-x)dt- \nabla f(x)^\top(y-x)\\
&=(y-x)^\top(\int_0^1 \nabla f(x+t(y-x))dt-\nabla f(x))\\
&=(y-x)^\top\int_0^1 (\nabla f(x+t(y-x))-\nabla f(x))dt\\
&\leq (y-x)^\top\int_0^1 \|\nabla f(x+t(y-x))-\nabla f(x)\|dt\\
&\leq (y-x)^\top\int_0^1 \frac{L}{2}\|t(y-x)\|dt\\
&=\frac{L}{2}\|y-x\|^2
\end{aligned}$$
Note we use Cauchy Schwartz inequality in line 4-5, and the definition of Lipschitz continuity of $\nabla f(x)$ in line 5-6.

Q.E.D.

Another **proof**: Since $f$ is convex, we can combine $f(y)\geq f(x)+\nabla f(x)^\top(y-x)$ and $f(x)\geq f(y)+\nabla f(y)^\top(x-y)$ to get 
$$
(\nabla f(x)-\nabla f(y))^\top (x-y) \geq 0
$$
We then construct $g(x)=\frac{L}{2}x^\top x-f(x)$.
$$\begin{aligned}
&\nabla g(x)-\nabla g(y) = L(x-y)-(\nabla f(x)-\nabla f(y))\\
& \Rightarrow (\nabla g(x)-\nabla g(y))^\top (x-y)\\
&=L(x-y)^\top(x-y)-(\nabla f(x)-\nabla f(y))^\top (x-y) \geq 0
\end{aligned}$$
we can claim $g(x)$ is convex, which means 
$$\begin{aligned}
& g(y) \geq g(x) + \nabla g(x)^\top (y-x)\\
& \Rightarrow \frac{L}{2} y^\top y-f(y) \geq \frac{L}{2} x^\top x-f(x)+(Lx-\nabla f(x))^\top (y-x)\\
& \Rightarrow f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2 \geq f(y)
\end{aligned}$$

Q.E.D.

A third **proof**: According to $\nabla f$ is Lipschitz continuous with constant $L$, $\|\nabla f(x)-\nabla f(y)\|_2\leq L\|x-y\|_2$, let $x=y+\Delta$ where $\Delta \rightarrow 0$, we have $\nabla^2 f \preceq LI$.

For any $z \in dom f$,
$$\begin{aligned}
& \nabla^2 f(z) \preceq LI\\
& \Rightarrow (x-y)^\top (\nabla^2 f(z)-LI) (x-y)\leq 0\\
& \Rightarrow (x-y)^\top \nabla^2 f(z) (x-y)\leq L \|x-y\|^2
\end{aligned}$$

We can use Taylorâ€™s Remainder Theorem:
$$\begin{aligned}
&f(y)= f(x) + \nabla f(x)^\top(y-x)+ \frac{1}{2}(x-y)^\top \nabla^2 f(x)(x-y) \\
&\leq f(x) + \nabla f(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2
\end{aligned}$$

Q.E.D.

Note that the second and third proof are identical since when a convex function $f$ is twice differentiable, its Hessian is positive semidefite:
$$
\nabla^2 f(x) \succeq 0.
$$
Therefore, $\nabla^2 g \succeq 0 \rightarrow \nabla^2 f \preceq LI$.

**Theorem**: Gradient descent with a fixed step-size $t\leq 1/L$ satisifies
$$
f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}
$$

This theorem is the main theoretical result of gradient descent, which guarantees the algorithm has convergence rate $O(1/k)$. In other words, the algorithm guarantees to approximate the $\epsilon$-accuracy, i.e. $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\epsilon)$ iterations.

**Proof**:
Assume $y=x-t\nabla f(x)$ of above lemma, and a fixed step size $0<t\leq 1/L$,
$$
f(y)\leq f(x)-t\|\nabla f(x)\|^2+\frac{Lt^2}{2}\|\nabla f(x)\|^2\\
=f(x)-t(1-\frac{Lt}{2})\|\nabla f(x)\|^2
$$
Denote $x^+=x-t\nabla f(x)$, and note $Lt\leq 1$, we have
$$\begin{aligned}
&f(x^+)\leq f(x)-\frac{t}{2}\|\nabla f(x)\|^2\\
&\leq f^*+f(x)^\top(x-x^*)-\frac{t}{2}\|\nabla f(x)\|^2\\
&=f^*+\frac{1}{2t}(\|x-x^*\|^2-\|x-x^*-t\nabla f(x)\|^2)\\
&=f^*+\frac{1}{2t}(\|x-x^*\|^2-\|x^+-x^*\|^2)
\end{aligned}$$
Note we apply the convexity inequality in 1-2 line.

We now define $x=x^{(i-1)}$ and $x^+=x^{(i)}$, and sum over iterations:
$$\begin{aligned}
&\sum_{i=1}^k(f(x^{(i)})-f^*)\leq \frac{1}{2t}\sum_{i=1}^k(\|x^{(i-1)}-x^*\|^2-\|x^{(i)}-x^*\|^2)\\
&=\frac{1}{2t}(\|x^{(0)}-x^*\|^2-\|x^{(i)}-x^*\|^2)\\
&\leq \frac{1}{2t}(\|x^{(0)}-x^*\|^2)
\end{aligned}$$

Since $f(x^{(k)})$ is non-increasing, 
$$
f(x^{(k)})-f^* \leq
\frac{1}{k}\sum_{i=1}^k(f(x^{(i)})-f^*)
\leq \frac{1}{2tk}(\|x^{(0)}-x^*\|^2)
$$

Q.E.D.

# Subgradient method
Sometimes a function is not differentiable at certain points, such as $\|x\|_1$ at 0. We still want to use gradient based approaches. Being similar to the differentiable convex functions, we have

**Definition**: A subgradient of a convex function $f$ is $g\in \mathbb{R}^n$ such that
$$
f(y)\geq f(x)+g^\top(y-x)
$$
**Definition**: A subdifferential is a closed convex set of all subgradients of a convex function $f$:
$$
\partial f(x)=\{g\in \mathbb{R}^n: g \textrm{ is a subgradient of }f \textrm{ at } x\}
$$

The subgradient descent: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat
$$x^{(k)}=x^{(k-1)}-t_k g^{(k-1)},\ k=1,2,3...$$
until some stopping criteria is satisified. Where $g^{(k-1)}$ is any subgradient of $f$ at $x^{(k-1)}$. $t_k$ is the step size at $k$-th iteration, which can be picked as fixed or flexible.

## Convergence analysis
We assume the convex function $f$ satistifes:

- $f$ is Lipschitz continuous with constant $L$.
  $$
  \|f(x)-f(y)\|_2\leq L\|x-y\|_2,\ \forall x,y \in \mathbb{R}^n.
  $$
- $\|x^{(1)}-x^*\|_2\leq R$ which means it is bounded.

**Theorem**: Subgradient gradient descent with a fixed step-size $t$ satisifies
$$
\lim_{k\to\infty} f(x^{(k)}_{best})\leq f(x^*)+\frac{L^2t}{2}
$$


**Proof**: 
$$\begin{aligned}
\|x^{(k+1)}-x^*\|^2&=\|x^{(k)}-t_k g^{(k)}-x^*\|^2\\
&=\|x^{(k)}-x^*\|^2-2t_k g^{(k)\top} (x^{(k)}-x^*)+t_k^2 \|g^{(k)}\|^2
\end{aligned}$$

By the definition of subgradient we have
$$\begin{aligned}
&f(x^*)\geq f(x)+g^\top(x^*-x)\\
&\Rightarrow -g^{(k)\top} (x^{(k)}-x^*) \leq f(x^*)- f(x^{(k)})
\end{aligned}$$

we now have
$$\begin{aligned}
\|x^{(k+1)}-x^*\|^2 & \leq \|x^{(k)}-x^*\|^2+2t_k(f(x^*)- f(x^{(k)}))+t_k^2 \|g^{(k)}\|^2\\
& \leq \|x^{(1)}-x^*\|^2+2\sum_{i=1}^kt_i(f(x^*)- f(x^{(i)}))+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
& \leq R^2+2\sum_{i=1}^kt_i(f(x^*)- f(x^{(i)}))+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
\end{aligned}$$
Since $\|x^{(k+1)}-x^*\|^2 \geq 0$
$$\begin{aligned}
&2\sum_{i=1}^kt_i(f(x^{(i)})-f(x^*))\leq R^2+\sum_{i=1}^kt_i^2 \|g^{(i)}\|^2\\
\Rightarrow & 2(\sum_{i=1}^kt_i)(f(x^{(k)}_{best})-f(x^*))\leq R^2+\sum_{i=1}^kt_i^2 L^2
\end{aligned}$$

- For a fixed step size:
$$
\lim_{k\to \infty} \frac{R^2+kt^2 L^2}{2tk}=\frac{L^2t}{2}
$$
- For a specific step size: $t_i=R/(L\sqrt{k})$
$$
\frac{R^2+\sum_{i=1}^kt_i^2 L^2}{2(\sum_{i=1}^kt_i)}=\frac{RL}{\sqrt{k}}
$$

The algorithm has convergence rate $O(1/\sqrt{k})$ or get $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\epsilon^2)$ iterations.

Q.E.D.

# Generalized gradient descent
Before we start, refer to the third proof of Lemma in gradient descent, we use the quadratic approximation for a convex function $f$ at certain point. 
$$\begin{aligned}
f(x) + \nabla f(x)^\top(z-x)+ \frac{1}{2}(z-x)^\top \nabla^2 f(x)(z-x)
\end{aligned}$$

By replacing the $\nabla^2 g$ by $I/t$ we can derive the radient update rule (with fixed step size $t$)
$$\begin{aligned}
& x^+=\arg\min_z f(x) + \nabla f(x)^\top(z-x)+ \frac{1}{2t}\|z-x\|^2\\
& \Rightarrow x^+= x-t \nabla f(x)
\end{aligned}$$

The gradient descent can be generalized to work for a class of general functions:
$$
f(x)=g(x)+h(x)
$$
where $g(x)$ is convex and differentiable while $h(x)$ is convex but not differentiable. It is common in machine learning since a dozen of classical models can be expressed as "loss+regularization" when the regularization may not be differentiable everywhere (such as $\ell_1$-norm).

Here we can do the same quadratic approximation for differentiable part $g(x)$, but keep the other part $h(x)$ unchanged for now, the update rule is
$$\begin{aligned}
&x^+ =\arg\min_z g(x) + \nabla g(x)^\top(z-x)+ \frac{1}{2t}\|z-x\|^2+h(z)\\
& = \arg\min_z \frac{1}{2t}\|z-(x-t\nabla g(x))\|^2+h(z)
\end{aligned}$$

Note the operator 
$$\begin{aligned}
{\rm prox}_h(x)=\arg\min_z \frac{1}{2}\|z-x\|^2+h(z)
\end{aligned}$$
is also called proximal mapping or prox-operator of convex function $h$. Examples include:

 - $h$ is indicator function of closed convex set $C$, then ${\rm prox}_h$ is a projection on $C$:
 $$\begin{aligned}
 {\rm prox}_h(x)=P_C(x)=\arg\min_{u\in C}\|u-x\|^2
 \end{aligned}$$
 
 - $h(x)=\|x\|_1$, ${\rm prox}_h$ is the soft-threshold operator:
$$\begin{aligned}
{\rm prox}_h(x)_i=\left\{
  \begin{array}{ll}
    x_i-1 & x_i\geq 1\\
    0 & |x_i|< 1\\
    x_i+1 & x_i\leq -1
  \end{array}
\right.
\end{aligned}$$

It is also called **proximal gradient desent**.

The algorithm: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat

$$\begin{aligned}
& x^+=x^{(k-1)}-t_k\nabla f(x^{(k-1)})\\
& x^{(k)}= {\rm prox}_h(x^+)
\end{aligned}$$
until some stopping criteria is satisified.

The updating rule can be rewritten in a similar way to gradient descent:
$$\begin{aligned}
x^{(k)}=x^{(k-1)}-t_kG_{t_k}(x^{(k-1)})
\end{aligned}$$
where
$$\begin{aligned}
G_t(x)=\frac{x-{\rm prox}_h(x-t\nabla g(x))}{t}
\end{aligned}$$

## Convergence analysis
Assume $g$ has the same properties as in previous section, and $h$ is convex and ${\rm prox}_h$ is cheap to calculate, then we have
**Lemma** The Lipschitz continuity of $\nabla g(x)$ leads to
$$
f(y)\leq g(x) + \nabla g(x)^\top(y-x)+ \frac{L}{2}\|x-y\|^2+h(y),\ \forall x,y
$$
The proof keeps the same.

**Theorem**: Generalized gradient descent with a fixed step-size $t\leq 1/L$ satisifies
$$
f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}
$$

It has the same convergence rate $O(1/k)$ as gradient descent.

**Proof**:
Assume $y=x^+=x-tG_t(x)$ of above lemma, and a fixed step size $0<t\leq 1/L$,
$$
f(x^+)\leq g(x)-t\nabla g(x)^\top G_t(x)+\frac{Lt^2}{2}\|G_t(x)\|^2+h(x-tG_t(x))
$$
Note in the updating rule
$$\begin{aligned}
x^+ = x-tG_t(x) &=\arg\min_z \frac{1}{2t}\|z-(x-t\nabla g(x))\|^2+h(z)\\
& = \arg\min_z \nabla g(x)^\top (z-x)+\frac{1}{2t}\|z-x\|^2+h(z)
\end{aligned}$$

Since $h(z)$ is not differentiable, the minimum occurs when
$$
\nabla g(x)+\frac{1}{t}(z-x)+v=0
$$
where $v\in \partial h(z)$.
Also note $z=x-tG_t(x)$. We get
$$\begin{aligned}
& \nabla g(x)-G_t(x)+v=0\\
& \Rightarrow G_t(x)-g(x) \in \partial h(x-tG_t(x))
\end{aligned}$$
Since $h$ is convex
$$\begin{aligned}
& h(x)\geq h(x-tG_t(x))+(G_t(x)-g(x))^\top tG_t(x)\\
& \Rightarrow h(x-tG_t(x)) \leq h(x)-t(G_t(x)-g(x))^\top G_t(x)
\end{aligned}$$

We carry on the beginning of this proof,
$$\begin{aligned}
&f(x^+)\leq g(x)-t\nabla g(x)^\top G_t(x)+\frac{Lt^2}{2}\|G_t(x)\|^2+h(x)-t(G_t(x)-g(x))^\top G_t(x)\\
& = f(x)+t(1-\frac{Lt}{2})\|G_t(x)\|^2\\
&\leq f(x)-\frac{t}{2}\|G_t(x)\|^2\\
\end{aligned}$$
We should stop here because everything keeps the same as in the proof of gradient descent, except that we replace $\nabla f(x)$ with $G_t(x)$.

Q.E.D.


# Acceleration

The generalized gradient descent (of course including the simple gradient descent) can be accelerated by using Nesterov's method. The idea is to use a linear combination of previous two steps instead of one, to compute a candidate solution.

The algorithm: start with an initial $x^{(0)}\in\mathbb{R}^n$, repeat

$$\begin{aligned}
& v=x^{(k-1)}+\frac{k-2}{k+1}(x^{(k-1)}-x^{(k-2)})\\
& x^{(k)}= {\rm prox}_h(v-t_k\nabla g(v))
\end{aligned}$$
until some stopping criteria is satisified.

**Theorem**: Accelerated gradient descent with a fixed step-size $t\leq 1/L$ satisifies
$$
f(x^{(k)})-f(x^*)\leq \frac{2\|x^{(0)}-x^*\|^2}{t(k+1)^2}
$$

It has the same convergence rate $O(1/k^2)$, or it gets $f(x^{(k)})-f(x^*)\leq \epsilon$ in $O(1/\sqrt{\epsilon})$ iterations.

Before we carry on, let's have a look at the performances of three different algrothms: subgradient ($O(1/\sqrt{k}$), proximal gradient ($O(1/k)$) and accelerated gradient $O(1/k^2)$.



# Endnote
This post is about the first-order methods, based on gradient descent. The covergence rates:

|  Algorithm   | Function  | Condition | Convergence rate   |
|-----+--------------+-----------------+---------|
| Gradient desent  | $f(x)$   | convex, differentiable, L-smooth  | $O(1/k)$ |
| Subgradient desent | $f(x)=g(x)+h(x)$ | $f$: convex, differentiable, L-smooth, $g$:convex  | $O(1/\sqrt{k})$ |
| Generalized gradient desent | $f(x)=g(x)+h(x)$ | $f$: convex, differentiable, L-smooth, $g$:convex   | $O(1/k)$ |
| Accelerated gradient desent    | $f(x)=g(x)+h(x)$   | $f$: convex, differentiable, L-smooth, $g$:convex | $O(1/k^2)$ |

# Reference

Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

L. Vandenberghe's Lecture notes: http://www.seas.ucla.edu/~vandenbe/236C

Strongly Convex: http://www.stronglyconvex.com/blog/gradient-descent.html

Geoff Gordon and Ryan Tibshirani. https://www.cs.cmu.edu/~ggordon/10725-F12

Marco Tulio Ribeiro's Blog: https://homes.cs.washington.edu/~marcotcr/blog/gradient-descent

<!-- Sebastien Bubeck's Blog: https://blogs.princeton.edu/imabandit/archives -->

--
Last update: `r Sys.setlocale("LC_TIME", "English"); format(Sys.Date(), "%B %d, %Y")`
